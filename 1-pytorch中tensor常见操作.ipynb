{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "tensor([1., 2., 3.])"
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch  \n",
        "import numpy as np \n",
        "a= [1,2,3] \n",
        "t = torch.Tensor(a)  # 数组转tensor\n",
        "t "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([2., 3.])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t1 = torch.Tensor(np.array([2,3]))\n",
        "t1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "4"
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t1 = torch.tensor([1,1,1])  # 数组转tensor\n",
        "t2 = torch.Tensor(2,3)  # 随机2*3矩阵\n",
        "t3 = torch.ones(2,1)  # 填充1\n",
        "t4 = torch.zeros(1,2)  # 填充0\n",
        "t5 = torch.eye(2,2)  # 返回二维张量，对角线上是1，其它地方是0  \n",
        "\n",
        "t5.size()         # tensor的size() \n",
        "torch.numel(t5)  # tensor中元素进行计数  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### 其他常见  tensor操作  \n",
        "1. 统计tensor中总共有多少元素 torch.numel(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "###  初始化操作    \n",
        "1. torch自己进行初始化  \n",
        "2. 通过 numpy、list进行转换  \n",
        "3. 借助已有tensor进行转换 \n",
        "\n",
        "#### 不懂\n",
        "* torch.tensor 总是会复制 data, 如果你想避免复制，可以使 torch.Tensor. detach()，如果是从 numpy 中获得数据，那么你可以用 torch.from_numpy(), 注from_numpy() 是共享内存的\n",
        "* array([2,3]) 和 [2,3]的区别"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "tensor([[2., 2.]])\n"
        }
      ],
      "source": [
        "# torch自己进行初始化  \n",
        "t1 = torch.Tensor(2,3)  \n",
        "t2 = torch.tensor([1,1,1])\n",
        "\n",
        "t3 = torch.ones(2,1)\n",
        "t4 = torch.zeros(1,3) \n",
        "t5 = torch.eye(2,2)  \n",
        "\n",
        "t6 = torch.rand(2,2)     # 均匀分布 \n",
        "t7 = torch.randn(t6.size())    # 标准正态分布   \n",
        "# # print(type(t7.size()))    # class  torch.Size\n",
        "\n",
        "\n",
        "# 借助已有 tensor,主要是size\n",
        "t8 = torch.empty(2,3)  # 空 tensor ，里面是无穷接近于0的数字\n",
        "t9 = torch.empty_like(t8)  # 张量返回一个与输入大小相同的未初始化的张量\n",
        "t10 = torch.full((1,2),2)  # 返回大小为sizes,单位值为fill_value的矩阵\n",
        "t11 = torch.full_like(t9,1)  # 返回与input相同size，单位值为fill_value的矩阵  \n",
        "print(t10)\n",
        "\n",
        "# 通过numpy 和 list    \n",
        "# Tensor 和  np.array() 之间共享内存，也就意味着: 其中一个发生变化，另外一个也会发生变化  \n",
        "list1 = [1,2,3,4]\n",
        "np1 = np.array([2,3])\n",
        "# print(np1)\n",
        "t12 = torch.Tensor(list1)  \n",
        "t13 = torch.Tensor(np.array([1,2,3,4,5,6]))\n",
        "t14 = torch.from_numpy(np1)  \n",
        "# print(t14)\n",
        "np1 = np.array([1,2,3,4,5,6,8])\n",
        "np2 = t14.numpy()     # tensor转换为 numpy     |   list好像是不可以直接进行转换  \n",
        "# np2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "###  计算操作 - 基本数学运算    \n",
        " 1. 加法\n",
        "2. 减法  \n",
        " 3. 元素乘积  \n",
        " 4. 除法  \n",
        "5.乘法\n",
        " 5.1 二维矩阵乘法 \n",
        "   5.2 多维矩阵相乘 \n",
        " 6. 幂运算  \n",
        " 7. 开方运算 \n",
        " 8. 指数和对数运算  \n",
        " 9. 近似值运算 \n",
        "\n",
        "#### 问题\n",
        "* torch.mm & torch.matmul &torch.mul 的区别\n",
        "https://blog.csdn.net/da_kao_la/article/details/87484403\n",
        "* 广播就是增加维度再删除？\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "tensor([[3., 3.],\n        [3., 3.]]) tensor([[9., 9.],\n        [9., 9.]]) tensor([[27., 27.],\n        [27., 27.]])\ntensor(3.1400) tensor(3.) tensor(4.) tensor(3.) tensor(0.1400) tensor(3.)\n1\n"
        },
        {
          "data": {
            "text/plain": "(tensor([[ 6.0464,  3.5679, 12.3470],\n         [ 1.0466, 10.0537,  7.9185]]),\n tensor(12.3470),\n tensor(1.0466),\n tensor(6.0464),\n tensor([[10.0000, 10.0000, 12.3470],\n         [10.0000, 10.0537, 10.0000]]),\n tensor([[ 6.0464,  3.5679, 10.0000],\n         [ 3.0000, 10.0000,  7.9185]]))"
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "###  计算操作 - 基本数学运算    \n",
        "# 1. 加法\n",
        "# 2. 减法  \n",
        "# 3. 元素乘积  \n",
        "# 4. 除法  \n",
        "# 5.乘法\n",
        "#   5.1 二维矩阵乘法 \n",
        "#   5.2 多维矩阵相乘 \n",
        "# 6. 幂运算  \n",
        "# 7. 开方运算 \n",
        "# 8. 指数和对数运算  \n",
        "# 9. 近似值运算 \n",
        "# 10. 裁剪运算： 对于tensor中的元素进行范围过滤，常用于梯度 裁剪  \n",
        "\n",
        "a1 = torch.rand(3,4) \n",
        "a2 = torch.rand(4)  \n",
        "a3 = a1+a2 \n",
        "a4 = torch.add(a1,a2)   # 加法\n",
        "\n",
        "a5 = a1-a2\n",
        "a6 = torch.sub(a1,a2)   # 减法 \n",
        "\n",
        "a7 = a1*a2\n",
        "a8 = torch.mul(a1,a2)  # 3.元素乘法 \n",
        "\n",
        "a9 = a1/a2\n",
        "a10 = torch.div(a1,a2)  # 4.元素除法\n",
        "\n",
        "\n",
        "b1 = torch.ones(2,1) \n",
        "b2 = torch.ones(1,2)\n",
        "\n",
        "b3 = torch.mm(b1,b2)\n",
        "b4 = torch.matmul(b1,b2)\n",
        "b5 = b1@b2                 # 5.1 三种 二维矩阵相乘 \n",
        "\n",
        "# 多维矩阵乘法： dim>2的tensor,定义其矩阵乘法仅在最后两个维度上面进行，要求前面的维度必须保持一致，\n",
        "# 就像矩阵索引一样，并且 运算只有  torch.matmul()  \n",
        "# 前面的‘矩阵索引维度’如果符合 Broadcasting机制，也会自动做广播，然后相乘  \n",
        "c1 = torch.ones(2,2,2,1)  \n",
        "c2 = torch.rand(1,2,1,2)\n",
        "c3 = torch.matmul(c1,c2)               # 多维tensor的矩阵乘法\n",
        "c1,c2,c3,c3.size()  \n",
        "\n",
        "\n",
        "c4 = torch.full([2,2],3)\n",
        "c5 = c4 ** 2 \n",
        "c6 = c4.pow(3)                       # 6.幂运算 \n",
        "print(c4,c5,c6)\n",
        "\n",
        "c7 = c5 ** (0.5)\n",
        "c8 = c5.sqrt()                       # 7. tensor的开方运算\n",
        "c9 = c5.rsqrt()\n",
        "c7,c8,c9\n",
        "\n",
        "# 指数与对数运算  \n",
        "# 注意： log是以 自然对数为底数的， 以2为底用 log2, 以10为底用log10  \n",
        "\n",
        "d1 = torch.ones(2,2)  \n",
        "d2 = torch.exp(d1)\n",
        "d3 = torch.log(d2)\n",
        "\n",
        "d4 = torch.full([2,2],10)\n",
        "d5 = torch.log10(d4)                 # 9. 取对数运算\n",
        "\n",
        "d1,d2,d3,d5   \n",
        "\n",
        "\n",
        "d6 = torch.tensor(3.14) \n",
        "d7 = d6.floor()   # 取下 3.\n",
        "d8 = d6.ceil()    # 取上  4.\n",
        "d9 = d6.trunc()   # 取整数  3.                # 其他取值运算\n",
        "d10 = d6.frac()    # 取小数  0.1400\n",
        "d11 = d6.round()    # 四舍五入  3.\n",
        "\n",
        "\n",
        "# 裁剪运算： 对 tensor中的元素进行范围过滤，不符合条件的可以把它变换到 范围内部(边界)上，\n",
        "# 常用于梯度剪裁(Gradient Clipping)，即在发生梯度离散  或者  梯度爆炸 时对于梯度的处理，实际应用 \n",
        "# 可以查看梯度的(L2范数)模来看看需不需要处理： W.grad.norm(2)  \n",
        "grad = torch.rand(2,3) * 15 \n",
        "g1 = grad.max()\n",
        "g2 = grad.min()  \n",
        "g3 = grad.median()  \n",
        "\n",
        "g4 = grad.clamp(10) \n",
        "g5 = grad.clamp(3,10)   # 10. 梯度裁剪\n",
        "\n",
        "grad,g1,g2,g3,g4,g5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### pytorch 的 tensor 的统计属性  \n",
        "1. 范数  \n",
        "2. 常用操作: \n",
        "2.1 均值、累加、最小、最大、累积  \n",
        "2.2 直接使用max | min 配合 dim参数也可以获得最值索引，同时得到最值的具体值  \n",
        "2.3 使用 keepdim=True可以保持应有的dim,即仅仅是 将求最值的那个 dim的size变成了1，返回的   \n",
        "结果是符合原 tensor语义的。  \n",
        "2.4 取 前K大/ 前K小/ 第K小 的概率值及其索引  \n",
        "2.5 比较操作  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "e1 = torch.full([8],1)\n",
        "e2 = e1.reshape([2,4])\n",
        "e3 = e1.reshape([2,2,2]) \n",
        "\n",
        "e4 = e1.norm(1) \n",
        "e5 = e2.norm(1)   # L1范数: 所有元素绝对值求和  \n",
        "\n",
        "e6 = e1.norm(2) \n",
        "e7 = e2.norm(2)   # L2范数: 平方和开根号  \n",
        "\n",
        "e8 = e1.norm(1,dim=0) \n",
        "e9 = e2.norm(2,dim=1)   # 根据指定维度 求范数  \n",
        "\n",
        "\n",
        "f1 = torch.arange(8).reshape(2,4).float()  \n",
        "f2 = f1.mean()    # 均值\n",
        "f3 = f1.sum()     # 累加\n",
        "f4 = f1.min()     # 最小 \n",
        "f5 = f1.max()     # 最大\n",
        "f6 = f1.prod()    # 累积\n",
        "\n",
        "f7 = f1.argmax()\n",
        "f8 = f1.argmin()   # 整个tensor打平之后的 最大、最小值 索引 \n",
        "# 上述的  argmax() 、 argmin() 操作会默认将 tensor打平后取最大值索引、  \n",
        "#  最小值索引，如果不希望tensor打平，而是求 给定维度上的索引，需要指定在哪一个\n",
        "# 维度上面求 最大值索引 和 最小值索引。  \n",
        "\n",
        "f9 = torch.rand(2,2,2)\n",
        "f10 = f9.argmax(dim =0)\n",
        "f10_ = f9.max(dim=0)\n",
        "f10_1 = f9.max(dim=0,keepdim=True)   # 三种方法后面好好再看看\n",
        "# f11 = f9.argmax(dim=1)\n",
        "# f12 = f9.argmax(dim=2)\n",
        "\n",
        "# topk概率选择 \n",
        "# 使用 topk代替max可以完成更加灵活的需求， largest=False, kthvalue\n",
        "# 还可以取第k小的概率值及其索引  \n",
        "# 不给定dim则默认最后一维，largest=False代表取小\n",
        "\n",
        "h1 = torch.randn(2,10)  \n",
        "h2 = h1.topk(3,dim=1)\n",
        "h3 = h1.topk(3,dim=1,largest=False)  # 概率最小 【前】 3 \n",
        "h4 = h1.kthvalue(8,dim=1)    # 求 【第】 8概率小的 \n",
        "h1,h2,h3,h4   \n",
        "\n",
        "\n",
        "\n",
        "# 比较操作  \n",
        "k1 = torch.randn(2,3)\n",
        "k2 = torch.randn(2,3)  \n",
        "k3 = k1>0   \n",
        "k4 = torch.gt(k1,0)   # 两种方法判断 K1是否大于0，是 对应位置返回1，否 0\n",
        "\n",
        "k5 = a != 0              # 比较每个位置是否 等于  0\n",
        "\n",
        "k6 = torch.equal(k1,k2)\n",
        "k3,k4,k5\n",
        "k6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### 维度变换操作    \n",
        "1. 拼接  torch.cat(seq,dim,out) \n",
        "2. 维度扩展  \n",
        "3. 压缩  \n",
        "4. 转置   \n",
        "5. 重复   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### 链接  \n",
        "1. https://blog.csdn.net/TH_NUM/article/details/83088915   \n",
        "2. https://blog.csdn.net/weixin_44613063/article/details/89521464 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.5189, -1.8552,  0.1157],\n",
            "        [ 1.0232,  0.9460, -0.9362],\n",
            "        [ 0.5189, -1.8552,  0.1157],\n",
            "        [ 1.0232,  0.9460, -0.9362],\n",
            "        [ 0.5189, -1.8552,  0.1157],\n",
            "        [ 1.0232,  0.9460, -0.9362]])\n"
          ]
        }
      ],
      "source": [
        "# 向量拼接  \n",
        "# 1.torch.cat()  \n",
        "# 2. torch.stack()\n",
        "\n",
        "# torch.cat(seq,dim=0,out=None)\n",
        "# 除去seq中tensor的该维度，然后按照指定的维度进行拼接，但是总的维度数目不变，拼接的内容可能是 tesnor可能是里面的最后一个维度的数字\n",
        "import torch\n",
        "h_1 = torch.randn(2,3)\n",
        "h_2 = torch.cat((h_1,h_1,h_1),0) \n",
        "print(h_2)  # [2,9]  [6,3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2, 3]) tensor([[[ 1,  2,  3],\n",
            "         [11, 22, 33]],\n",
            "\n",
            "        [[ 4,  5,  6],\n",
            "         [44, 55, 66]]], dtype=torch.int32)\n",
            "torch.Size([2, 2, 3]) tensor([[[ 1,  2,  3],\n",
            "         [ 4,  5,  6]],\n",
            "\n",
            "        [[11, 22, 33],\n",
            "         [44, 55, 66]]], dtype=torch.int32)\n",
            "torch.Size([2, 3, 2]) tensor([[[ 1,  4],\n",
            "         [ 2,  5],\n",
            "         [ 3,  6]],\n",
            "\n",
            "        [[11, 44],\n",
            "         [22, 55],\n",
            "         [33, 66]]], dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "# torch.stack((Tensor),dim)    \n",
        "\n",
        "# 多出一个维度，按照指定维度把 tensor中对应的内容，叠加起来增加一个维度\n",
        "h_3 = torch.IntTensor([[1,2,3],[11,22,33]])\n",
        "h_4 = torch.IntTensor([[4,5,6],[44,55,66]])\n",
        "\n",
        "h_5 =torch.stack([h_3,h_4],0)\n",
        "h_6 =torch.stack([h_3,h_4],1)\n",
        "h_7 =torch.stack([h_3,h_4],2)\n",
        "print(h_5.size(),h_5)   # [2,2,3]\n",
        "print(h_6.size(),h_6)\n",
        "print(h_7.size(),h_7)\n",
        "# h_5, dim = 0时， c = [ a, b]\n",
        "# h_6, dim =1 时， d = [ [a[0] , b[0] ] , [a[1], b[1] ] ]\n",
        "# h_7, dim = 2 时， e=[[[a[0][0],b[0][0]],[a[0][1],b[0][1]],[a[0][2],b[0][2]]],[[a[1][0],b[1][0]],[a[1][1],b[0][1]],[a[1][2],b[1][2]]]]e = [ [ [ a[0][0], b[0][0] ] , [ a[0][1], b[0][1] ] , [ a[0][2],b[0][2] ] ] , [ [ a[1][0], b[1][0] ] , [ a[1][1], b[0][1] ] , [ a[1][2],b[1][2] ] ] ]e=[[[a[0][0],b[0][0]],[a[0][1],b[0][1]],[a[0][2],b[0][2]]],[[a[1][0],b[1][0]],[a[1][1],b[0][1]],[a[1][2],b[1][2]]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "h_8的size: torch.Size([3, 1])\ntensor([[[1., 1., 1., 1., 1.],\n         [2., 2., 2., 2., 2.],\n         [3., 3., 3., 3., 3.]],\n\n        [[1., 1., 1., 1., 1.],\n         [2., 2., 2., 2., 2.],\n         [3., 3., 3., 3., 3.]]]) torch.Size([2, 3, 5])\n"
        }
      ],
      "source": [
        "# 扩大向量  \n",
        "# 1. torch.Tensor.expand(*sizes) --> Tensor   \n",
        "# 返回张量的一个新视图，可以将张量的单个维度扩大为 更大的尺寸  \n",
        "# 张量也可以扩大为 更高维，新增加的维度将 附在前面。  \n",
        "# 扩大张量不需要分配新内存，仅仅是新建一个张量的视图。 任意一个一维张量在不分配新内存情况下都可以扩展为任意的维度。  \n",
        "# 传入 -1 意味着 维度扩大不涉及这个维度  \n",
        "# 参数: * sizes(torch.Size or int )  - 想要扩展的目标维度  \n",
        "h_8 = torch.Tensor([[1],[2],[3]])  \n",
        "print('h_8的size:',h_8.size())\n",
        "h_9 = h_8.expand(2,3,5)   # 注意不管怎么扩展 原始tensor的第0维度数值不能变化，要不然 tensor不知道扩展什么。 也就是广播  即3不能变\n",
        "h_10 = h_8.expand(2,3,1)\n",
        "print(h_9,h_9.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k_1.size() :  torch.Size([2, 1, 2, 1, 2])\n",
            "k_2:  torch.Size([2, 2, 2]) tensor([[[0., 0.],\n",
            "         [0., 0.]],\n",
            "\n",
            "        [[0., 0.],\n",
            "         [0., 0.]]])\n",
            "k_3: torch.Size([2, 1, 2, 1, 2]) tensor([[[[[0., 0.]],\n",
            "\n",
            "          [[0., 0.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0., 0.]],\n",
            "\n",
            "          [[0., 0.]]]]])\n",
            "k_4:  torch.Size([2, 2, 1, 2]) tensor([[[[0., 0.]],\n",
            "\n",
            "         [[0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0.]],\n",
            "\n",
            "         [[0., 0.]]]])\n"
          ]
        }
      ],
      "source": [
        "# 压缩张量  \n",
        "# torch.squeeze(input,dim=None) --> Tensor   \n",
        "# 除去输入张量input中数值为1的维度，并返回新的张量。  \n",
        "# 如果输入张量的维度形状为(A*1*B*C*1*D),那么输出张量的形状为(A*B*C*D)  \n",
        "# 当通过dim参数进行指定维度时，维度压缩操作只会在指定的维度上进行。 如果输入张量形状为（A*1*B）,squeeze(input,0)会保持张量的维度\n",
        "# 不变，只有在执行squeeze(input,1)时，输入张量的形状会被压缩至(A*B)  \n",
        "# 如果一个张量只有一个维度，那么它不会受到上述方法的影响  \n",
        "# 输出的张量与原张量共享内存，如果改变其中的一个，另一个也会改变  \n",
        "# 参数: \n",
        "#  * input(Tensor) - 输入张量 \n",
        "#  * dim(int,optional) - 如果给定，则只会在给定维度压缩  \n",
        "#  * out(Tensor,optional) -输出张量  \n",
        "\n",
        "k_1 = torch.zeros(2,1,2,1,2)  \n",
        "print('k_1.size() : ',k_1.size()) \n",
        "k_2 = torch.squeeze(k_1) \n",
        "print('k_2: ',k_2.size(),k_2)  \n",
        "k_3 = torch.squeeze(k_1,0)\n",
        "print('k_3:',k_3.size(),k_3)  \n",
        "k_4 = torch.squeeze(k_1,1)\n",
        "print('k_4: ',k_4.size(),k_4)   \n",
        "\n",
        "\n",
        "# torch.unsqueeze(input,dim)  \n",
        "# 返回一个新的张量，对于 输入的指定位置插入维度1  \n",
        "# 返回张量与输入张量共享内存。 \n",
        "# 如果 dim 为负数，则将会被转为 dim+input.dim()+1  \n",
        "k_4_1 = k_1.unsqueeze(0) \n",
        "print('k_4_1: ',)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k_6: tensor([[1., 2., 3.],\n",
            "        [1., 2., 3.],\n",
            "        [1., 2., 3.]])\n",
            "k_7:  torch.Size([7]) tensor([1, 2, 3, 4, 5, 6, 7])\n",
            "k_8: torch.Size([3, 3]) tensor([[1, 2, 3],\n",
            "        [3, 4, 5],\n",
            "        [5, 6, 7]])\n"
          ]
        }
      ],
      "source": [
        "# 重复张量 -- \n",
        "# 1. torch.Tensor.repeat(*sizes)    \n",
        "# 沿着指定的维度重复张量。 不同于 expand()方法，本函数复制的是张量中的数据。 \n",
        "# 参数: size(torch.Size or int) -沿着每一维度重复的次数  \n",
        "k_5 = torch.Tensor([1,2,3])\n",
        "k_6 = k_5.repeat(3,1) \n",
        "print('k_6:',k_6)\n",
        "\n",
        "# 2. torch.Tensor.unfold(dim,size,step) --> Tensor   \n",
        "# 返回一个新的张量，其中元素 复制于原有张量在 dim维度上面的数据，重复复制size次，复制时的步进值为 step。 \n",
        "# 参数: \n",
        "#    * dim(int)  - 目标维度 \n",
        "#    * size(int) - 复制重复的次数(展开维度)  \n",
        "#    * step(int) - 步长\n",
        "\n",
        "k_7 = torch.arange(1,8)\n",
        "print('k_7: ',k_7.size(),k_7)  \n",
        "k_8 = k_7.unfold(0,3,2) \n",
        "print('k_8:',k_8.size(),k_8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "l_2:  torch.Size([3, 3]) tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.],\n",
            "        [7., 8., 9.]])\n",
            "l_3:  torch.Size([3, 1]) tensor([[1.],\n",
            "        [4.],\n",
            "        [7.]])\n"
          ]
        }
      ],
      "source": [
        "# 缩小张量  \n",
        "# 1. torch.Tensor.narrow(dim,start,length)\n",
        "# 返回一个经过缩小后的张量。 操作的维度是由 dim指定。 缩小范围是从 start 开始到 start+length。 \n",
        "# 执行本方法的张量与 返回的张量共享相同的底层结构  \n",
        "# 参数: \n",
        "#  * dim(int) - 要进行缩小的维度  \n",
        "#  * start(int) - 开始维度索引  \n",
        "#  * length(int) - 缩小持续的长度  \n",
        "\n",
        "l_1 = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "l_2 = l_1.narrow(0, 0, 3)  \n",
        "print('l_2: ',l_2.size(),l_2)  \n",
        "\n",
        "l_3 = l_1.narrow(1,0,1) \n",
        "print('l_3: ',l_3.size(),l_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "l_4 torch.Size([4, 4]) tensor([[-0.3817,  2.4836, -0.4313, -0.5117],\n",
            "        [-0.5709,  0.9594,  2.1554, -1.2182],\n",
            "        [ 0.3045, -0.3556,  0.6329, -0.3100],\n",
            "        [ 1.1020,  0.8372, -0.9051,  1.9131]])\n",
            "l_5:  torch.Size([16]) tensor([-0.3817,  2.4836, -0.4313, -0.5117, -0.5709,  0.9594,  2.1554, -1.2182,\n",
            "         0.3045, -0.3556,  0.6329, -0.3100,  1.1020,  0.8372, -0.9051,  1.9131])\n",
            "l_6:  torch.Size([2, 2, 4]) tensor([[[-0.3817,  2.4836, -0.4313, -0.5117],\n",
            "         [-0.5709,  0.9594,  2.1554, -1.2182]],\n",
            "\n",
            "        [[ 0.3045, -0.3556,  0.6329, -0.3100],\n",
            "         [ 1.1020,  0.8372, -0.9051,  1.9131]]])\n"
          ]
        }
      ],
      "source": [
        "# 张量变形  \n",
        "# 1. torch.Tensor.view(*args) --> Tensor  \n",
        "# 返回一个具有 相同数据 但是不同形状的张量  \n",
        "# 返回的张量必须与 原张量有相同的数据 和 相同的元素个数，但是可以有不同的尺寸    \n",
        "# 一个 tensor 必须是连续的 contiguous() 才能被查看。\n",
        "# 参数： args(torch.Size or int)  - 理想的指定尺寸  \n",
        "\n",
        "l_4 = torch.randn(4,4)  \n",
        "print('l_4',l_4.size(),l_4)   \n",
        "\n",
        "l_5 = l_4.view([16]) \n",
        "print('l_5: ',l_5.size(),l_5)  \n",
        "\n",
        "l_6 = l_4.view([2,2,4]) \n",
        "print('l_6: ',l_6.size(),l_6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "l_8:  torch.Size([2, 2]) tensor([[1., 2.],\n",
            "        [3., 4.]])\n"
          ]
        }
      ],
      "source": [
        "# 重设张量尺寸 \n",
        "# 1. torch.resize_()    \n",
        "# 将张量的尺寸调整为指定的大小。 如果元素个数比当前的内存大小大，就将底层存储 大小，调整为 与新元素数目一致的大小。  \n",
        "# 如果元素个数比当前内存小，则底层存储不会被改变。 原来张量中被保存下来的元素将保持不变，但是新内存不会被初始化。   \n",
        "# 参数： sizes(torch.Size or int)  -需要调整的大小  \n",
        "l_7 = torch.Tensor([[1,2],[3,4],[5,6]])  \n",
        "l_8 = l_7.resize_(2,2)\n",
        "print('l_8: ',l_8.size(),l_8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "m_1 torch.Size([2, 3, 5]) tensor([[[-0.6380,  1.2446, -0.9995, -1.0774, -1.0818],\n",
            "         [ 2.2983, -2.1731, -0.6929, -0.2860, -0.7544],\n",
            "         [ 0.6070, -0.4391, -0.9510, -1.3038, -0.3321]],\n",
            "\n",
            "        [[ 1.4086,  0.1324, -0.3574, -0.5572,  1.5325],\n",
            "         [-0.5728, -0.7109, -2.5166, -0.2381,  0.2539],\n",
            "         [-0.8991,  0.0039, -0.7443, -0.5701, -0.4112]]])\n",
            "m_2:  torch.Size([5, 2, 3]) tensor([[[-0.6380,  2.2983,  0.6070],\n",
            "         [ 1.4086, -0.5728, -0.8991]],\n",
            "\n",
            "        [[ 1.2446, -2.1731, -0.4391],\n",
            "         [ 0.1324, -0.7109,  0.0039]],\n",
            "\n",
            "        [[-0.9995, -0.6929, -0.9510],\n",
            "         [-0.3574, -2.5166, -0.7443]],\n",
            "\n",
            "        [[-1.0774, -0.2860, -1.3038],\n",
            "         [-0.5572, -0.2381, -0.5701]],\n",
            "\n",
            "        [[-1.0818, -0.7544, -0.3321],\n",
            "         [ 1.5325,  0.2539, -0.4112]]])\n",
            "n_1:  torch.Size([2, 4, 3]) tensor([[[ 0.5896,  0.6844,  1.1751],\n",
            "         [-0.2296, -0.8836, -1.8135],\n",
            "         [-0.2536, -0.1605, -0.8355],\n",
            "         [-1.1912,  1.4416,  0.4441]],\n",
            "\n",
            "        [[ 1.4102, -1.2788, -1.6070],\n",
            "         [ 1.5739, -1.1812,  0.8881],\n",
            "         [-0.5947,  0.0697,  0.3946],\n",
            "         [ 1.5943,  1.2507,  0.2114]]])\n",
            "n_2:  torch.Size([2, 3, 4]) tensor([[[ 0.5896, -0.2296, -0.2536, -1.1912],\n",
            "         [ 0.6844, -0.8836, -0.1605,  1.4416],\n",
            "         [ 1.1751, -1.8135, -0.8355,  0.4441]],\n",
            "\n",
            "        [[ 1.4102,  1.5739, -0.5947,  1.5943],\n",
            "         [-1.2788, -1.1812,  0.0697,  1.2507],\n",
            "         [-1.6070,  0.8881,  0.3946,  0.2114]]])\n"
          ]
        }
      ],
      "source": [
        "# 置换张量维度  \n",
        "# 1. torch.Tensor.permute(*dims)  \n",
        "# 将执行本方法的张量的维度置换。   \n",
        "# 直接在方法中填入各个维度的索引，张量就会交换指定维度的尺寸，不限于两两交换。\n",
        "# 参数: dim(int) - 指定换位顺序  \n",
        "m_1 = torch.randn(2,3,5)  \n",
        "print('m_1',m_1.size(),m_1)\n",
        "\n",
        "m_2 = m_1.permute(2,0,1)  \n",
        "print('m_2: ',m_2.size(),m_2)  \n",
        "\n",
        "\n",
        "# torch.transpose()  \n",
        "# torch.transpose(input,dim0,dim1) --> Tensor \n",
        "# 返回输入矩阵 input 转置。  交换维度 din0 - dim1. \n",
        "# 输入张量与 输出张量共享内存。  \n",
        "\n",
        "n_1 = torch.randn(2,4,3) \n",
        "print('n_1: ',n_1.size(),n_1)  \n",
        "n_2 = torch.transpose(n_1,1,2)\n",
        "print('n_2: ',n_2.size(),n_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 查看张量单个元素的字节数   \n",
        "# torch.Tensor.element_size() --> int \n",
        "# 查看某类型张量单个元素的字节数  \n",
        "torch.FloatTensor().element_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# 矩阵转置 \n",
        "# torch.t(input) \n",
        "# 输入一个矩阵(2维张量),并转置 0-1 维度，可以看作 函数 transpose(input,0,1)的简写函数  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}